{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(text):\n",
    "    for word, replacement in corrections.items():\n",
    "        text = text.replace(word, replacement)\n",
    "    return text\n",
    "\n",
    "#Frequency functions\n",
    "def entertainment_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = entertainment_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def exercise_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = exercise_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def family_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = family_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def food_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = food_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def people_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = people_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def work_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = work_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def school_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = school_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def shopping_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = shopping_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "def pets_frequency(text):\n",
    "    lowered_text = text.lower()\n",
    "    matches = pets_pattern.findall(lowered_text)\n",
    "    return len(matches)\n",
    "\n",
    "def sentence_preprocessing(sentence):\n",
    "    #Replace \"`\" with \"'\"\n",
    "    sentence = sentence.replace(\"`\",\"'\")\n",
    "    #Replace contractions\n",
    "    sentence = contractions.fix(sentence)\n",
    "    sentence = re.sub(r\"\\w+\\.com\",'', sentence)\n",
    "    #Remove URLs\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    #Remove numbers\n",
    "    sentence = \"\".join([i for i in sentence if not i.isdigit()])\n",
    "    #Remove punctuations except \"$\", \"-\", and \"'\"\n",
    "    sentence = \"\".join([i for i in sentence if i not in string.punctuation or i==\"$\" or i == \"-\" or i==\"'\" or i==\"_\"])\n",
    "    #Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    #Remove stop words\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in updated_stopwords or token.lower()==\"won\"]\n",
    "    return tokens\n",
    "def text_correction(word):\n",
    "    corrections = {\n",
    "    \"unde's\": 'uncle',\n",
    "    \"uncle's\": 'uncle',\n",
    "    \"b'day\": 'birthday',\n",
    "    \"mother's\": 'mother',\n",
    "    \"year's\": \"year\",\n",
    "    \"bus,\": \"bus\",\n",
    "    'february,': 'february',\n",
    "    \"children's\": 'children',\n",
    "    \"daughter's\": 'daughter',\n",
    "    \"did't\": \"did not\",\n",
    "    'ndonating': 'donate',\n",
    "    \"god's\": 'god',\n",
    "    \"sister's\": 'sister',\n",
    "    \"sisters's\": 'sister',\n",
    "    \"parent's\": 'parent',\n",
    "    \"brother's\": 'brother',\n",
    "    'thrones0': 'thrones',\n",
    "        \"n't\": 'not'\n",
    "}\n",
    "    if word in corrections:\n",
    "        return corrections[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_tags = [\"n\", \"v\", \"a\", \"r\", \"s\", \"p\", \"i\", \"c\", \"u\", \"x\"]\n",
    "    for pos_tag in pos_tags:\n",
    "        pos_counts[pos_tag] = len([item for item in probable_part_of_speech if item.pos() == pos_tag])\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def rightTypes(ngram):\n",
    "    for word in ngram:\n",
    "        if word in stopwords.words('english') or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def bigram_noun(bigram):\n",
    "    first_tag = get_part_of_speech(bigram[0])\n",
    "    second_tag = get_part_of_speech(bigram[1])\n",
    "    if first_tag not in ['a','n'] and second_tag != 'n':\n",
    "        return False\n",
    "    return True\n",
    "def replace_bigram(hm_text):\n",
    "    for bigram in bigrams:\n",
    "        hm_text = hm_text.replace(bigram, '_'.join(bigram.split()))\n",
    "    return hm_text\n",
    "\n",
    "def noun_verb(x):\n",
    "    tag = nltk.pos_tag(x)\n",
    "    filtered_tokens = [word[0] for word in tag if word[1].startswith(\"N\")]\n",
    "    return filtered_tokens\n",
    "\n",
    "def topic_counts(df):\n",
    "    topic_counts_list = dict()\n",
    "    for topic in topic_list:\n",
    "        topic_counts_list[topic] = (df[topic]!=0).sum()\n",
    "    topic_counts = dict(sorted(topic_counts_list.items(), key=lambda x: x[1], reverse=True))\n",
    "    return topic_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
